{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t9Q5XWnBaaTo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing Stanford LLMs Lecture Notes**"
      ],
      "metadata": {
        "id": "NHSPrQlrdtdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove LaTeX commands enclosed within \\(\\) and $$\n",
        "    text = re.sub(r'\\\\\\((.*?)\\\\\\)', '', text)\n",
        "    text = re.sub(r'\\$\\$(.*?)\\$\\$', '', text)\n",
        "    # Remove LaTeX commands enclosed within \\[ and \\]\n",
        "    text = re.sub(r'\\\\\\[(.*?)\\\\\\]', '', text)\n",
        "    # Remove other LaTeX commands with backslashes\n",
        "    text = re.sub(r'\\\\([^\\\\]+){(.*?)}', '', text)\n",
        "    # Remove LaTeX commands without backslashes\n",
        "    text = re.sub(r'\\\\(\\w+)', '', text)\n",
        "    return text\n",
        "\n",
        "# Function to parse lecture notes from a single chapter page\n",
        "def parse_chapter_page(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Find the HTML element(s) containing the chapter text\n",
        "        # You need to inspect the HTML structure of the page to identify the correct element(s)\n",
        "        chapter_text_elements = soup.find_all('div', class_='main-content-wrap')  # Example: <div class=\"chapter-text\">\n",
        "        # Extract text from the chapter text elements\n",
        "        chapter_text = '\\n'.join([element.get_text() for element in chapter_text_elements])\n",
        "        # Preprocess the chapter text to remove LaTeX commands\n",
        "        chapter_text = preprocess_text(chapter_text)\n",
        "        # Remove extra whitespace and newline characters\n",
        "        chapter_text = ' '.join(chapter_text.split())\n",
        "        return chapter_text\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# Function to parse lecture notes from the main page containing links to chapters\n",
        "def parse_main_page(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Find all links to individual chapter pages\n",
        "        chapter_links = soup.find_all('a', href=True)\n",
        "        # Extract URLs of individual chapter pages excluding links with #\n",
        "        chapter_urls = [link['href'] for link in chapter_links if '#' not in link['href']]\n",
        "        return chapter_urls\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Main function to parse lecture notes from all chapters\n",
        "def parse_all_lecture_notes(main_url):\n",
        "    # Parse main page to get URLs of individual chapter pages\n",
        "    chapter_urls = parse_main_page(main_url)\n",
        "    if not chapter_urls:\n",
        "        print(\"No chapter URLs found.\")\n",
        "        return\n",
        "\n",
        "    # Parse lecture notes from each chapter page\n",
        "    lecture_notes = {}\n",
        "    for url in chapter_urls:\n",
        "        print(url)\n",
        "        chapter_text = parse_chapter_page(url)\n",
        "        if chapter_text:\n",
        "            lecture_notes[url] = chapter_text\n",
        "\n",
        "    return lecture_notes\n",
        "\n",
        "# URL of the main page containing links to chapters\n",
        "main_page_url = \"https://stanford-cs324.github.io/winter2022/lectures/\"\n",
        "\n",
        "# Parse lecture notes from all chapters\n",
        "parsed_lecture_notes = parse_all_lecture_notes(main_page_url)\n",
        "\n",
        "# Display the parsed lecture notes (for demonstration purposes)\n",
        "for url, text in parsed_lecture_notes.items():\n",
        "    print(f\"Chapter URL: {url}\")\n",
        "    print(f\"Chapter Text: {text[:200]}...\")  # Displaying a portion of the text\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "chapter_text_data = parsed_lecture_notes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq5czYVfbVSP",
        "outputId": "53c391f9-fc12-45dc-deaf-1ebfb69d9c68"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://stanford-cs324.github.io/winter2022/\n",
            "https://stanford-cs324.github.io/winter2022/\n",
            "https://stanford-cs324.github.io/winter2022/calendar/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/data/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/security/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/training/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/parallelism/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/\n",
            "https://stanford-cs324.github.io/winter2022/paper-reviews/\n",
            "https://stanford-cs324.github.io/winter2022/paper-discussions/\n",
            "https://stanford-cs324.github.io/winter2022/projects/\n",
            "https://github.com/pmarsceill/just-the-docs\n",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/data/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/security/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/training/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/parallelism/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/\n",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/\n",
            "Chapter Text: CS324 - Large Language ModelsThe field of natural language processing (NLP) has been transformed by massive pre-trained language models. They form the basis of all state-of-the-art systems across a wi...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/calendar/\n",
            "Chapter Text: Calendar Behavior of large language modelsMon Jan 3IntroductionLecturePercy LiangWhy does this course exist?Language modelsOverview of the courseWed Jan 5CapabilitiesLecture DiscussionPercy LiangAdapt...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/\n",
            "Chapter Text: CS324 lecture notes (Winter 2022)As CS324 is a new class, the lecture notes are being constructed on the fly. New content will be added as the quarter progresses.Table of contents Introduction Capabil...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\n",
            "Chapter Text: LecturesIntroduction Welcome to CS324! This is a new course on understanding and developing large language models.What is a language model?A brief historyWhy does this course exist?Structure of this c...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\n",
            "Chapter Text: LecturesCapabilities In this lecture, we will explore the capabilities of GPT-3, the canonical large language model. We will closely follow the benchmarks from the GPT-3 paper, which include:standard ...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/harms-1/\n",
            "Chapter Text: LecturesHarms I In this lecture, we will begin our exploration of the harms of large language models. In this course, we will cover several of these harms, largely following the foundation models repo...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\n",
            "Chapter Text: LecturesHarms II In the last lecture, we started discussing the harms (negative impacts) on people who use systems powered by large language models. We call these behavioral harms because these are ha...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/data/\n",
            "Chapter Text: LecturesDataSo far, we’ve talked about the behavior (capabilities and harms) of large language models. Now, we peel open the first layer of the onion and start discussing how these models are construc...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/security/\n",
            "Chapter Text: LecturesSecurityThis lecture was delivered via slides. A link is available on Canvas and pdfs are here Further readingExtracting Training Data from Large Language Models. Nicholas Carlini, Florian Tra...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/legality/\n",
            "Chapter Text: LecturesLegalityIn this lecture, we will discuss what the law has to say about the development and deployment of large language models.As with previous lectures, for example the one on social bias, mu...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/modeling/\n",
            "Chapter Text: LecturesModeling We started this course by analyzing a language model as a black box:Then we looked at the training data of large language models (e.g., The Pile):In this lecture, we will open up the ...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/training/\n",
            "Chapter Text: LecturesTraining Last lecture, we talked about the model architecture for large language models (e.g., the Transformer). In this lecture, we will discuss how to train large language models.Objective f...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/parallelism/\n",
            "Chapter Text: LecturesParallelismThis lecture was delivered via whiteboard and slides. A draft of the lecture is provided here. Further supporting discussion on parallelism more generally is given here. Further rea...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/\n",
            "Chapter Text: LecturesScaling lawsThis lecture was delivered via slides. Powerpoint slides at Canvas and PDFs are available here Further readingScaling Laws for Neural Language Models. J. Kaplan, Sam McCandlish, T....\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/\n",
            "Chapter Text: LecturesSelective architectures Recall from the modeling lecture that the core interface of a neural language model is an encoder that maps token sequences to contextual embeddings:GPT-3 is a neural l...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/adaptation/\n",
            "Chapter Text: LecturesAdaptation By only prompting language models (.e.g, in-context learning), we can already do some tasks.However, prompting doesn’t work on the full range of downstream tasks (e.g., NLI, QA, con...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/lectures/environment/\n",
            "Chapter Text: LecturesEnvironmental impact In this lecture, ask the question: what is the environmental impact of large language models?Climate change. On one hand, we’ve all heard about the very serious dangers of...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/paper-reviews/\n",
            "Chapter Text: Paper reviewsThe goal of the panel reviews is to critically read and analyze the paper(s). Reviewing is a central practice in the academic community; our goal for the course is for you to have a bette...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/paper-discussions/\n",
            "Chapter Text: Paper discussionsThe goal of the panel discussion is to analyze the paper from a variety of different vantage points. During each panel discussion, there is a panel of 4-5 students, each with an assig...\n",
            "--------------------------\n",
            "Chapter URL: https://stanford-cs324.github.io/winter2022/projects/\n",
            "Chapter Text: ProjectsProject 1Project 2...\n",
            "--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing the table of model architectures**"
      ],
      "metadata": {
        "id": "cdl9h974d1sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# URL of the GitHub readme file\n",
        "git_url = \"https://github.com/Hannibal046/Awesome-LLM#milestone-papers\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(git_url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "article = soup.find(\"article\")\n",
        "# Find the table containing milestone papers within the <article> tag\n",
        "table = article.find(\"table\")\n",
        "\n",
        "# Extract table rows\n",
        "rows = table.find_all(\"tr\")\n",
        "\n",
        "# Extract data from each row\n",
        "milestone_papers = []\n",
        "for row in rows:\n",
        "    columns = row.find_all(\"td\")\n",
        "    if len(columns) >= 3:  # Check if the row contains at least three columns\n",
        "        paper = {\n",
        "\n",
        "            \"Date\": columns[0].text.strip(),\n",
        "            \"Title\": columns[3].text.strip(),\n",
        "            \"Authors\": columns[2].text.strip(),\n",
        "            \"Conference\": columns[4].text.strip(),\n",
        "        }\n",
        "        milestone_papers.append(paper)\n",
        "\n",
        "# Print milestone papers\n",
        "for paper in milestone_papers:\n",
        "    print(\"Date:\", paper[\"Date\"])\n",
        "    print(\"Title:\", paper[\"Title\"])\n",
        "    print(\"Authors:\", paper[\"Authors\"])\n",
        "    print(\"Conference:\", paper[\"Conference\"])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDPBsOdhqbWg",
        "outputId": "e5a92533-4727-4985-b58c-90f89cb905c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2017-06\n",
            "Title: Attention Is All You Need\n",
            "Authors: Google\n",
            "Conference: NeurIPS\n",
            "\n",
            "Date: 2018-06\n",
            "Title: Improving Language Understanding by Generative Pre-Training\n",
            "Authors: OpenAI\n",
            "Conference: \n",
            "\n",
            "Date: 2018-10\n",
            "Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
            "Authors: Google\n",
            "Conference: NAACL\n",
            "\n",
            "Date: 2019-02\n",
            "Title: Language Models are Unsupervised Multitask Learners\n",
            "Authors: OpenAI\n",
            "Conference: \n",
            "\n",
            "Date: 2019-09\n",
            "Title: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n",
            "Authors: NVIDIA\n",
            "Conference: \n",
            "\n",
            "Date: 2019-10\n",
            "Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "Authors: Google\n",
            "Conference: JMLR\n",
            "\n",
            "Date: 2019-10\n",
            "Title: ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n",
            "Authors: Microsoft\n",
            "Conference: SC\n",
            "\n",
            "Date: 2020-01\n",
            "Title: Scaling Laws for Neural Language Models\n",
            "Authors: OpenAI\n",
            "Conference: \n",
            "\n",
            "Date: 2020-05\n",
            "Title: Language models are few-shot learners\n",
            "Authors: OpenAI\n",
            "Conference: NeurIPS\n",
            "\n",
            "Date: 2021-01\n",
            "Title: Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n",
            "Authors: Google\n",
            "Conference: JMLR\n",
            "\n",
            "Date: 2021-08\n",
            "Title: Evaluating Large Language Models Trained on Code\n",
            "Authors: OpenAI\n",
            "Conference: \n",
            "\n",
            "Date: 2021-08\n",
            "Title: On the Opportunities and Risks of Foundation Models\n",
            "Authors: Stanford\n",
            "Conference: \n",
            "\n",
            "Date: 2021-09\n",
            "Title: Finetuned Language Models are Zero-Shot Learners\n",
            "Authors: Google\n",
            "Conference: ICLR\n",
            "\n",
            "Date: 2021-10\n",
            "Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\n",
            "Authors: HuggingFace et al.\n",
            "Conference: ICLR\n",
            "\n",
            "Date: 2021-12\n",
            "Title: GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\n",
            "Authors: Google\n",
            "Conference: ICML\n",
            "\n",
            "Date: 2021-12\n",
            "Title: WebGPT: Browser-assisted question-answering with human feedback\n",
            "Authors: OpenAI\n",
            "Conference: \n",
            "\n",
            "Date: 2021-12\n",
            "Title: Improving language models by retrieving from trillions of tokens\n",
            "Authors: DeepMind\n",
            "Conference: ICML\n",
            "\n",
            "Date: 2021-12\n",
            "Title: Scaling Language Models: Methods, Analysis & Insights from Training Gopher\n",
            "Authors: DeepMind\n",
            "Conference: \n",
            "\n",
            "Date: 2022-01\n",
            "Title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
            "Authors: Google\n",
            "Conference: NeurIPS\n",
            "\n",
            "Date: 2022-01\n",
            "Title: LaMDA: Language Models for Dialog Applications\n",
            "Authors: Google\n",
            "Conference: \n",
            "\n",
            "Date: 2022-01\n",
            "Title: Solving Quantitative Reasoning Problems with Language Models\n",
            "Authors: Google\n",
            "Conference: NeurIPS\n",
            "\n",
            "Date: 2022-01\n",
            "Title: Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\n",
            "Authors: Microsoft&NVIDIA\n",
            "Conference: \n",
            "\n",
            "Date: 2022-03\n",
            "Title: Training language models to follow instructions with human feedback\n",
            "Authors: OpenAI\n",
            "Conference: \n",
            "\n",
            "Date: 2022-04\n",
            "Title: PaLM: Scaling Language Modeling with Pathways\n",
            "Authors: Google\n",
            "Conference: \n",
            "\n",
            "Date: 2022-04\n",
            "Title: An empirical analysis of compute-optimal large language model training\n",
            "Authors: DeepMind\n",
            "Conference: NeurIPS\n",
            "\n",
            "Date: 2022-05\n",
            "Title: OPT: Open Pre-trained Transformer Language Models\n",
            "Authors: Meta\n",
            "Conference: \n",
            "\n",
            "Date: 2022-05\n",
            "Title: Unifying Language Learning Paradigms\n",
            "Authors: Google\n",
            "Conference: ICLR\n",
            "\n",
            "Date: 2022-06\n",
            "Title: Emergent Abilities of Large Language Models\n",
            "Authors: Google\n",
            "Conference: TMLR\n",
            "\n",
            "Date: 2022-06\n",
            "Title: Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\n",
            "Authors: Google\n",
            "Conference: \n",
            "\n",
            "Date: 2022-06\n",
            "Title: Language Models are General-Purpose Interfaces\n",
            "Authors: Microsoft\n",
            "Conference: \n",
            "\n",
            "Date: 2022-09\n",
            "Title: Improving alignment of dialogue agents via targeted human judgements\n",
            "Authors: DeepMind\n",
            "Conference: \n",
            "\n",
            "Date: 2022-10\n",
            "Title: Scaling Instruction-Finetuned Language Models\n",
            "Authors: Google\n",
            "Conference: \n",
            "\n",
            "Date: 2022-10\n",
            "Title: GLM-130B: An Open Bilingual Pre-trained Model\n",
            "Authors: Tsinghua\n",
            "Conference: ICLR\n",
            "\n",
            "Date: 2022-11\n",
            "Title: Holistic Evaluation of Language Models\n",
            "Authors: Stanford\n",
            "Conference: \n",
            "\n",
            "Date: 2022-11\n",
            "Title: BLOOM: A 176B-Parameter Open-Access Multilingual Language Model\n",
            "Authors: BigScience\n",
            "Conference: \n",
            "\n",
            "Date: 2022-11\n",
            "Title: Galactica: A Large Language Model for Science\n",
            "Authors: Meta\n",
            "Conference: \n",
            "\n",
            "Date: 2022-12\n",
            "Title: OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization\n",
            "Authors: Meta\n",
            "Conference: \n",
            "\n",
            "Date: 2023-01\n",
            "Title: The Flan Collection: Designing Data and Methods for Effective Instruction Tuning\n",
            "Authors: Google\n",
            "Conference: ICML\n",
            "\n",
            "Date: 2023-02\n",
            "Title: LLaMA: Open and Efficient Foundation Language Models\n",
            "Authors: Meta\n",
            "Conference: \n",
            "\n",
            "Date: 2023-02\n",
            "Title: Language Is Not All You Need: Aligning Perception with Language Models\n",
            "Authors: Microsoft\n",
            "Conference: \n",
            "\n",
            "Date: 2023-03\n",
            "Title: PaLM-E: An Embodied Multimodal Language Model\n",
            "Authors: Google\n",
            "Conference: ICML\n",
            "\n",
            "Date: 2023-03\n",
            "Title: GPT-4 Technical Report\n",
            "Authors: OpenAI\n",
            "Conference: \n",
            "\n",
            "Date: 2023-04\n",
            "Title: Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n",
            "Authors: EleutherAI et al.\n",
            "Conference: ICML\n",
            "\n",
            "Date: 2023-05\n",
            "Title: Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision\n",
            "Authors: CMU et al.\n",
            "Conference: NeurIPS\n",
            "\n",
            "Date: 2023-05\n",
            "Title: PaLM 2 Technical Report\n",
            "Authors: Google\n",
            "Conference: \n",
            "\n",
            "Date: 2023-05\n",
            "Title: RWKV: Reinventing RNNs for the Transformer Era\n",
            "Authors: Bo Peng\n",
            "Conference: EMNLP\n",
            "\n",
            "Date: 2023-05\n",
            "Title: Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n",
            "Authors: Stanford\n",
            "Conference: Neurips\n",
            "\n",
            "Date: 2023-05\n",
            "Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
            "Authors: Google&Princeton\n",
            "Conference: NeurIPS\n",
            "\n",
            "Date: 2023-07\n",
            "Title: Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
            "Authors: Meta\n",
            "Conference: \n",
            "\n",
            "Date: 2023-10\n",
            "Title: Mistral 7B\n",
            "Authors: Mistral\n",
            "Conference: \n",
            "\n",
            "Date: 2023-12\n",
            "Title: Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
            "Authors: CMU&Princeton\n",
            "Conference: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Up the Environment Variable"
      ],
      "metadata": {
        "id": "mENCW8HzeB4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain --upgrade\n",
        "# !pip install langchain-experimental\n",
        "# !pip install langchain-openai\n",
        "# !pip install beautifulsoup4\n",
        "# !pip install faiss-cpu\n",
        "os.environ['OPENAI_API_KEY']=\"your-api-key\"\n",
        "# \"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\"\n",
        "#\"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\"\n",
        "#\"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\" #\"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\""
      ],
      "metadata": {
        "id": "Y33O9QnJmg6o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.documents import Document\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "my_list = list(chapter_text_data.values())\n",
        "llm = ChatOpenAI()\n",
        "embeddings = OpenAIEmbeddings()\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "\n",
        "# Create a Document object\n",
        "docs = []\n",
        "for link, text in chapter_text_data.items():\n",
        "    doc = Document(page_content=text, metadata={\"link\": link})\n",
        "    docs.append(doc)\n",
        "\n",
        "for paper in milestone_papers:\n",
        "    paper_text = f\"Date: {paper['Date']}\\nTitle: {paper['Title']}\\nAuthors: {paper['Authors']}\\nConference: {paper['Conference']}\"\n",
        "    doc = Document(page_content=paper_text , metadata={\"link\": git_url})\n",
        "    docs.append(doc)\n",
        "\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "tFVDLQX504uK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector.as_retriever()\n",
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "])\n",
        "\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    # MessagesPlaceholder(variable_name=\"context\"),\n",
        "])\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
        "chat_history = [HumanMessage(content=\"What are LLM models used for?\"), AIMessage(content=\"Text Generation and Language Translation\")]"
      ],
      "metadata": {
        "id": "QfPylOj-zQrG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1:**"
      ],
      "metadata": {
        "id": "u4ADG7fjgbp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"What are some milestone model architectures and papers in the last few years?\",\n",
        "})"
      ],
      "metadata": {
        "id": "yO-uS_9O5Rdf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GETTING CITATIONS\n",
        "for res in response[\"context\"]:\n",
        "    print(res.metadata.get('link'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7gHMFNzaFWg",
        "outputId": "c47f9e42-90c5-4736-d2ff-15c75f79dea7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/Hannibal046/Awesome-LLM#milestone-papers\n",
            "https://github.com/Hannibal046/Awesome-LLM#milestone-papers\n",
            "https://stanford-cs324.github.io/winter2022/\n",
            "https://github.com/Hannibal046/Awesome-LLM#milestone-papers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## GETTING RESPONSE\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJjMPhyc50dR",
        "outputId": "e7767261-fcff-44c1-ae03-a3f2e453eef4"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some milestone model architectures and papers in the last few years include:\n",
            "\n",
            "1. BERT (Bidirectional Encoder Representations from Transformers) - Devlin et al., 2018\n",
            "2. GPT-2 (Generative Pre-trained Transformer 2) - Radford et al., 2019\n",
            "3. T5 (Text-to-Text Transfer Transformer) - Raffel et al., 2019\n",
            "4. GPT-3 (Generative Pre-trained Transformer 3) - Brown et al., 2020\n",
            "5. RoBERTa (A Robustly Optimized BERT Approach) - Liu et al., 2019\n",
            "\n",
            "These models have significantly advanced the field of natural language processing and have been instrumental in various NLP tasks and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus Part 1: Citations:**"
      ],
      "metadata": {
        "id": "vYrdblJZggHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## COMBINED FUNCTION WHICH TAKES IN QUERY AND GENERATES THE RESPONSE\n",
        "def get_response(query):\n",
        "  response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": query,\n",
        "  })\n",
        "  chat_history.append(HumanMessage(content=query))\n",
        "  chat_history.append(AIMessage(content=response[\"answer\"]))\n",
        "  print(\"Response...\\n\")\n",
        "  print(response[\"answer\"])\n",
        "  print(\"citations..\\n\")\n",
        "  for res in response[\"context\"]:\n",
        "    print(res.metadata.get('link'))"
      ],
      "metadata": {
        "id": "IgH6obAgbyni"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2:**"
      ],
      "metadata": {
        "id": "Y09r-b0Fg_LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"What are the layers in a transformer block?\",\n",
        "})"
      ],
      "metadata": {
        "id": "O0WNZNcrkS0e"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuGMLJSf8v3n",
        "outputId": "f91b2a14-081d-492a-c976-58da5629d2bf"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a transformer block, there are typically two main layers:\n",
            "\n",
            "1. Self-Attention Layer: This layer allows each token in the sequence to interact with every other token in the sequence, capturing dependencies and relationships between different parts of the input sequence.\n",
            "\n",
            "2. Feed-Forward Layer: This layer processes each token independently through a series of fully connected neural networks, providing non-linear transformations to the input embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3:**"
      ],
      "metadata": {
        "id": "MlxZQXCwhDN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me about datasets used to train LLMs and how they’re cleaned.\",\n",
        "})"
      ],
      "metadata": {
        "id": "4cJM47ZS8yYF"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkE_rQAQ87vY",
        "outputId": "e059a635-829c-4fd6-85b7-8dc356436e37"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large language models (LLMs) are trained on datasets that contain vast amounts of text data from various sources like the web, news, Wikipedia, and fiction. These datasets are cleaned to remove noise, irrelevant information, and potentially harmful content. For example, the WebText dataset used to train GPT-2 was created by scraping outbound links with upvotes, filtering out Wikipedia content, and ensuring high-quality diverse text. OpenAI's OpenWebText dataset was created by extracting URLs from Reddit submissions, filtering out non-English text, and removing duplicates. The Colossal Clean Crawled Corpus (C4) used to train the T5 model involved removing \"bad words,\" code, and non-English text. Cleaning the datasets helps improve the quality and reliability of the training data for LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(HumanMessage(content=\"Tell me about datasets used to train LLMs and how they’re cleaned.\"))"
      ],
      "metadata": {
        "id": "EJBnodD-89_N"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus Part 2:**"
      ],
      "metadata": {
        "id": "v_ulH-mjhUmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 4:(Demonstrating follow ups)**"
      ],
      "metadata": {
        "id": "Zugrb0OvgV-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"What are some challenges encountered during cleaning of datasets, and how are these challenges addressed?\",\n",
        "})"
      ],
      "metadata": {
        "id": "D82-xKS6928-"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iycmvq1N-XdA",
        "outputId": "79922ca8-f2fb-401f-92c1-9b5f6f0aa953"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "During the cleaning of datasets used to train Large Language Models (LLMs), several challenges can be encountered. Some of these challenges include:\n",
            "\n",
            "1. **Data Quality**: Ensuring that the data is of high quality and relevant to the task at hand. Low-quality or irrelevant data can negatively impact the performance of the model.\n",
            "\n",
            "2. **Data Bias**: Datasets may contain biases that can lead to discriminatory or unfair outcomes when the model is deployed. Addressing bias in the data is crucial to ensure fair and equitable results.\n",
            "\n",
            "3. **Data Privacy**: Protecting sensitive information present in the data is essential to maintain privacy and confidentiality. Anonymizing or removing personally identifiable information is often necessary.\n",
            "\n",
            "4. **Data Duplication**: Duplicates in the dataset can skew the model's training and evaluation. Removing duplicates or handling them appropriately is important for accurate training.\n",
            "\n",
            "5. **Data Size**: Large datasets can be challenging to clean efficiently. Managing and cleaning massive amounts of data can be time-consuming and resource-intensive.\n",
            "\n",
            "These challenges are addressed through various techniques and processes, including:\n",
            "\n",
            "1. **Data Preprocessing**: This involves cleaning, normalizing, and transforming the data to make it suitable for training the model. Steps like removing irrelevant information, handling missing values, and standardizing formats are part of this process.\n",
            "\n",
            "2. **Data Augmentation**: Adding variations to the dataset by generating new samples from existing data can help improve model performance and reduce overfitting.\n",
            "\n",
            "3. **Data Sampling**: Balancing the dataset by sampling from different classes or categories can address imbalances and biases in the data.\n",
            "\n",
            "4. **Data Validation**: Checking the cleaned data for errors, inconsistencies, and biases through validation techniques to ensure the quality and integrity of the dataset.\n",
            "\n",
            "5. **Ethical Considerations**: Considering ethical implications, such as fairness, transparency, and accountability, when cleaning and preparing the data to ensure that the model's outcomes are unbiased and reliable.\n",
            "\n",
            "By addressing these challenges and implementing appropriate strategies, researchers and data scientists can create high-quality datasets that lead to well-performing Large Language Models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Thanks for answering.\",\n",
        "})"
      ],
      "metadata": {
        "id": "clC_vwJjCcX9"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EVl6EcXwE3IA",
        "outputId": "a69179ea-d247-4a58-a5f3-66ae2541b46d"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You're welcome! If you have any more questions or need further information, feel free to ask. I'm here to help!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interactive Conversation**"
      ],
      "metadata": {
        "id": "Ap53gPpxfXuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag=1\n",
        "conversation_history = []\n",
        "for history in chat_history:\n",
        "  if(type(history)==HumanMessage):\n",
        "    conversation_history.append(\"User : \"+history)\n",
        "  elif(type(history)==AIMessage):\n",
        "    conversation_history.append(\"AI : \"+history)\n",
        "  else:\n",
        "    conversation_history.append(\"User : \"+history)\n",
        "\n",
        "while(flag):\n",
        "  query = input()\n",
        "  if query=='-1':\n",
        "    break\n",
        "  response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": query,\n",
        "  })\n",
        "  chat_history.append(query)\n",
        "  print(\"Question asked: \\n\", query)\n",
        "  conversation_history.append(\"User : \"+query)\n",
        "  conversation_history.append(\"AI : \"+response[\"answer\"])\n",
        "  print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KDPIqcMTpm_",
        "outputId": "d110b9a4-3317-4e70-f805-f4038da4a46c"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are LLM's used for? \n",
            "Question asked: \n",
            "What are LLM's used for?\n",
            "Large Language Models (LLMs) are used for various natural language processing tasks such as text generation, language translation, sentiment analysis, question answering, summarization, and more. They are capable of understanding and generating human-like text based on the patterns and data they have been trained on.\n",
            "Can LLMs summarise data?\n",
            "Question asked: \n",
            "Can LLMs summarise data? \n",
            "Yes, Large Language Models (LLMs) are capable of summarizing data. They can be used for text summarization tasks where they condense longer pieces of text into shorter summaries while retaining the key information and meaning. This ability makes them useful for tasks like creating abstracts of articles, condensing reports, or generating brief overviews of documents.\n",
            "Which LLM should I use for my work?\n",
            "Question asked: \n",
            "Which LLM should I use for my work? \n",
            "It depends on the specific natural language processing task you need to perform. Different LLMs have been trained and specialized for various tasks. For example, GPT-3 is known for its text generation capabilities, while BERT is often used for tasks like question answering and sentiment analysis. Evaluate the strengths and weaknesses of each model based on your requirements to determine the best fit for your work.\n",
            "-1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus Part 3:**"
      ],
      "metadata": {
        "id": "k2bAlVt5hjq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversation Summarizer**"
      ],
      "metadata": {
        "id": "Wu2iKq_LfI5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Concatenate conversation history into a single text\n",
        "conversation_text = \"\\n\".join(conversation_history)\n",
        "\n",
        "# Generate summary of conversation\n",
        "summary = summarizer(conversation_text, max_length=100, min_length=30, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Print the summary\n",
        "print(\"Summary of conversation:\")\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BnjNh2FE5nx",
        "outputId": "16ac0e37-4675-4cbd-82e9-eccb336c6c60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of conversation:\n",
            " Large Language Models (LLMs) are used for various natural language processing tasks such as text generation, language translation, sentiment analysis, question answering, summarization, and more . Can be used for text summarization tasks where they condense longer pieces of text into shorter summaries while retaining the key information and meaning . Different LLMs have been trained and specialized for various tasks .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwgU8GYyUKOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}